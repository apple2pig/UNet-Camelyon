# ğŸš€ å¦‚ä½•è¿è¡Œ Skill - å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹è¿è¡Œä¼˜åŒ–æ¨ç† Skillã€‚

---

## ğŸ“ å‰ç½®å‡†å¤‡

### 1. ç¡®ä¿è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»

**Windows (Command Prompt)**:
```bash
.venv\Scripts\activate
```

**Windows (PowerShell)**:
```bash
.venv\Scripts\Activate.ps1
```

**Linux/Mac**:
```bash
source .venv/bin/activate
```

éªŒè¯æ¿€æ´»æˆåŠŸï¼ˆå‘½ä»¤è¡Œå‰ç¼€åº”æ˜¾ç¤º `.venv`ï¼‰:
```bash
(.venv) C:\Users\junyou.zhang\Desktop\Us\UNet-Camelyon>
```

### 2. ç¡®ä¿å·²å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 3. å‡†å¤‡æ¨¡å‹æ–‡ä»¶

ä½ éœ€è¦ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶ï¼š
- æ–‡ä»¶å: `UNet_17.pth` (æˆ–å…¶ä»–åç§°)
- ä½ç½®: é¡¹ç›®æ ¹ç›®å½•æˆ–ä»»æ„è·¯å¾„
- å¤§å°: çº¦ 100MB

---

## ğŸ¯ ä¸‰ç§è¿è¡Œæ–¹å¼

### æ–¹å¼ 1ï¸âƒ£: è¿è¡Œç¤ºä¾‹è„šæœ¬ï¼ˆæ¨èæ–°æ‰‹ï¼‰

**æœ€ç®€å•ï¼Œé€‚åˆå¿«é€Ÿä½“éªŒ**

#### æ­¥éª¤

1. **è¿›å…¥ skills æ–‡ä»¶å¤¹**
   ```bash
   cd skills
   ```

2. **ç¼–è¾‘é…ç½®** (æ‰“å¼€ `example_optimized_inference.py`)

   æ‰¾åˆ°è¿™äº›è¡Œå¹¶ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„:
   ```python
   # ç¬¬ 16 è¡Œ - æ¨¡å‹è·¯å¾„
   model_path='UNet_17.pth'        # æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„

   # ç¬¬ 31 è¡Œ - WSI æ–‡ä»¶è·¯å¾„
   wsi_path = '/Camelyon16/test_040.tif'  # æ”¹ä¸ºä½ çš„ WSI è·¯å¾„

   # ç¬¬ 58-59 è¡Œ - è¾“å…¥è¾“å‡ºç›®å½•
   input_dir = 'patch_path/'       # æ”¹ä¸ºä½ çš„è¡¥ä¸ç›®å½•
   output_dir = 'output_path/'     # æ”¹ä¸ºè¾“å‡ºç›®å½•
   ```

3. **è¿è¡Œè„šæœ¬**
   ```bash
   python example_optimized_inference.py
   ```

4. **æŸ¥çœ‹è¾“å‡º**
   - æ¨¡å‹åˆå§‹åŒ–ä¿¡æ¯
   - å¤„ç†è¿›åº¦æ¡
   - æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
   - è¾“å‡ºæ–‡ä»¶ä½ç½®

---

### æ–¹å¼ 2ï¸âƒ£: æ€§èƒ½åŸºå‡†æµ‹è¯•

**å¯¹æ¯”åŸå§‹vsä¼˜åŒ–æ€§èƒ½ï¼ŒæŸ¥çœ‹åŠ é€Ÿæ•ˆæœ**

#### æ­¥éª¤

1. **è¿›å…¥ skills æ–‡ä»¶å¤¹**
   ```bash
   cd skills
   ```

2. **ç¼–è¾‘é…ç½®** (æ‰“å¼€ `compare_inference_speed.py`)

   ä¿®æ”¹è¿™äº›å˜é‡:
   ```python
   # çº¦ 122 è¡Œ
   model_path = 'UNet_17.pth'  # æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
   device = 'cuda:0'            # GPU è®¾å¤‡é€‰æ‹©
   num_patches = 100            # æµ‹è¯•è¡¥ä¸æ•°é‡
   batch_size = 12              # æ‰¹å¤„ç†å¤§å°
   ```

3. **è¿è¡Œæµ‹è¯•**
   ```bash
   python compare_inference_speed.py
   ```

4. **æŸ¥çœ‹ç»“æœ**
   - è¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
   - ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨ (`inference_comparison.png`)
   - é€Ÿåº¦æå‡å€æ•°
   - æ—¶é—´èŠ‚çœç»Ÿè®¡

**ç¤ºä¾‹è¾“å‡º**:
```
==============================================================
PERFORMANCE BENCHMARK
==============================================================

[1] Original Method (Single inference)
  Average time: 42.35ms per image

[2] Optimized Method (Batch + FP16 + JIT)
  Average time: 7.68ms per image

==============================================================
SPEEDUP: 5.51x faster
Time saved: 34.67ms per image
==============================================================
```

---

### æ–¹å¼ 3ï¸âƒ£: ç¼–å†™è‡ªå·±çš„è„šæœ¬

**è‡ªå®šä¹‰ä½¿ç”¨ï¼Œæœ€å¤§çµæ´»æ€§**

#### åŸºç¡€æ¨¡æ¿

```python
# 1. å¯¼å…¥
from skills.inference_optimized import OptimizedInference
import torch

# 2. æ£€æµ‹è®¾å¤‡
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# 3. åˆå§‹åŒ–ä¼˜åŒ–å¼•æ“
engine = OptimizedInference(
    model_path='UNet_17.pth',      # ä½ çš„æ¨¡å‹è·¯å¾„
    device=device,
    use_fp16=True,                  # å¯ç”¨åŠç²¾åº¦
    use_jit=True,                   # å¯ç”¨ JIT ç¼–è¯‘
    batch_size=12,                  # æ ¹æ® GPU å†…å­˜è°ƒæ•´
    num_workers=4                   # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
)

# 4. ä½¿ç”¨å¼•æ“

# é€‰é¡¹ A: å¤„ç† WSI
engine.process_wsi_batched(
    wsi_path='your_slide.tif',
    patch_size=512,
    output_path='result.png',
    overlap=0  # 0 = æ— é‡å ï¼Œ32/64 = å¹³æ»‘è¾¹ç•Œ
)

# é€‰é¡¹ B: å¤„ç† patch ç›®å½•
engine.process_patches_directory(
    input_dir='patches/',
    output_dir='results/'
)

# é€‰é¡¹ C: å•ä¸ªé¢„æµ‹
from PIL import Image
patch = Image.open('single_patch.png').convert('RGB')
prediction = engine.predict_batch([patch])
print(f"Prediction shape: {prediction[0].shape}")

# é€‰é¡¹ D: å¯¼å‡º ONNX
engine.export_to_onnx('model.onnx')
```

#### ä¿å­˜ä¸ºæ–‡ä»¶å¹¶è¿è¡Œ

```bash
# 1. åˆ›å»ºè‡ªå·±çš„è„šæœ¬
notepad my_inference.py

# 2. ç²˜è´´ä¸Šé¢çš„æ¨¡æ¿ï¼Œä¿®æ”¹è·¯å¾„
# 3. ä¿å­˜æ–‡ä»¶

# 4. è¿è¡Œ
cd skills
python my_inference.py
```

---

## ğŸ› ï¸ å¸¸è§é…ç½®ç¤ºä¾‹

### ä¾‹å­ 1: å¿«é€Ÿå¤„ç†å° WSI

```python
engine = OptimizedInference(
    model_path='UNet_17.pth',
    device='cuda:0',
    batch_size=8,          # è¾ƒå°æ‰¹æ¬¡
    use_fp16=False         # ç¦ç”¨ FP16 ä¿è¯ç²¾åº¦
)

engine.process_wsi_batched(
    wsi_path='small_slide.tif',
    patch_size=256,        # è¾ƒå°è¡¥ä¸
    output_path='result.png',
    overlap=0
)
```

### ä¾‹å­ 2: å¤„ç†è¶…å¤§ WSI

```python
engine = OptimizedInference(
    model_path='UNet_17.pth',
    device='cuda:0',
    batch_size=16,         # å¤§æ‰¹æ¬¡
    use_fp16=True          # å¯ç”¨ FP16 åŠ é€Ÿ
)

engine.process_wsi_batched(
    wsi_path='huge_slide.tif',
    patch_size=512,
    output_path='result.png',
    overlap=64             # é‡å è¾¹ç•Œæ›´å¹³æ»‘
)
```

### ä¾‹å­ 3: CPU æ¨ç†

```python
engine = OptimizedInference(
    model_path='UNet_17.pth',
    device='cpu',          # ä½¿ç”¨ CPU
    use_fp16=False,        # CPU ä¸æ”¯æŒ FP16
    batch_size=2,          # æ‰¹æ¬¡è¾ƒå°
    use_jit=True           # JIT åœ¨ CPU ä¸Šä»æœ‰æ•ˆ
)

# å¤„ç†...
```

### ä¾‹å­ 4: å®æ—¶åº”ç”¨

```python
engine = OptimizedInference(
    model_path='UNet_17.pth',
    device='cuda:0',
    batch_size=1,          # å•å¼ å›¾åƒ
    use_fp16=True,
    use_jit=True
)

# å•ä¸ªé¢„æµ‹
from PIL import Image
patch = Image.open('real_time_patch.png').convert('RGB')
pred = engine.predict_batch([patch])
result_value = pred[0].max()  # è·å–é¢„æµ‹å€¼
```

---

## ğŸ“Š å‚æ•°è°ƒä¼˜

### GPU æ˜¾å­˜ä¸è¶³

```python
# åŸå§‹é…ç½®æŠ¥é”™: CUDA Out of Memory

# è§£å†³æ–¹æ¡ˆ 1: é™ä½æ‰¹å¤„ç†å¤§å°
engine = OptimizedInference(batch_size=4)  # ä» 12 æ”¹ä¸º 4

# è§£å†³æ–¹æ¡ˆ 2: ç¦ç”¨ FP16
engine = OptimizedInference(use_fp16=False)

# è§£å†³æ–¹æ¡ˆ 3: é™ä½è¡¥ä¸å¤§å°
engine.process_wsi_batched(patch_size=256)  # ä» 512 æ”¹ä¸º 256

# è§£å†³æ–¹æ¡ˆ 4: ç»„åˆæ–¹æ¡ˆ
engine = OptimizedInference(
    batch_size=4,
    use_fp16=False,
    use_jit=True
)
```

### æé«˜ç²¾åº¦

```python
engine = OptimizedInference(
    use_fp16=False,  # ä½¿ç”¨ FP32 (å®Œæ•´ç²¾åº¦)
    use_jit=True     # ä½†ä»ç„¶ä½¿ç”¨ JIT åŠ é€Ÿ
)

# æˆ–æ·»åŠ è¾¹ç•Œå¹³æ»‘
engine.process_wsi_batched(
    overlap=64  # å¢åŠ é‡å ä»¥è·å¾—å¹³æ»‘ç»“æœ
)
```

### æé«˜é€Ÿåº¦

```python
engine = OptimizedInference(
    batch_size=24,   # å¢åŠ æ‰¹å¤„ç†å¤§å°
    use_fp16=True,   # å¯ç”¨åŠç²¾åº¦
    use_jit=True,    # å¯ç”¨ JIT
    num_workers=8    # æ›´å¤šåŠ è½½çº¿ç¨‹
)

engine.process_wsi_batched(
    overlap=0  # æ— é‡å æœ€å¿«
)
```

---

## âš¡ æ€§èƒ½ç›‘æ§

### æ–¹æ³• 1: GPU ç›‘æ§

åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ:
```bash
# å®æ—¶ç›‘æ§ GPU ä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

æˆ–ä¸€æ¬¡æ€§æŸ¥çœ‹:
```bash
nvidia-smi
```

### æ–¹æ³• 2: CPU å’Œå†…å­˜ç›‘æ§

```bash
# Windows
tasklist | find "python"

# Linux
ps aux | grep python
```

### æ–¹æ³• 3: ç¨‹åºå†…ç›‘æ§

è„šæœ¬ä¼šè‡ªåŠ¨è¾“å‡º:
```
âœ“ Processing complete!
  Time elapsed: 45.23s
  Speed: 14.22 patches/sec
  Average time per patch: 70.35ms
```

---

## ğŸ› å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### âŒ é”™è¯¯ 1: æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°

```
FileNotFoundError: [Errno 2] No such file or directory: 'UNet_17.pth'
```

**è§£å†³æ–¹æ¡ˆ**:
```python
import os

# ä½¿ç”¨ç»å¯¹è·¯å¾„
model_path = os.path.abspath('../UNet_17.pth')
engine = OptimizedInference(model_path=model_path)

# æˆ–æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if os.path.exists('UNet_17.pth'):
    print("âœ“ æ¨¡å‹æ–‡ä»¶æ‰¾åˆ°")
else:
    print("âœ— æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°")
    print(f"å½“å‰ç›®å½•: {os.getcwd()}")
    print(f"ç›®å½•å†…å®¹: {os.listdir('.')}")
```

### âŒ é”™è¯¯ 2: CUDA Out of Memory

```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å‡å°æ‰¹å¤„ç†å¤§å°
engine = OptimizedInference(batch_size=4)

# 2. ç¦ç”¨ FP16
engine = OptimizedInference(use_fp16=False)

# 3. æ¸…ç©º GPU ç¼“å­˜
import torch
torch.cuda.empty_cache()

# 4. é‡å¯ Python ç¯å¢ƒ
```

### âŒ é”™è¯¯ 3: å¯¼å…¥é”™è¯¯

```
ModuleNotFoundError: No module named 'inference_optimized'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸­
cd C:\Users\junyou.zhang\Desktop\Us\UNet-Camelyon\skills

# æˆ–ä½¿ç”¨ç»å¯¹å¯¼å…¥
import sys
sys.path.insert(0, 'C:\\Users\\junyou.zhang\\Desktop\\Us\\UNet-Camelyon\\skills')
from inference_optimized import OptimizedInference
```

### âŒ é”™è¯¯ 4: WSI æ–‡ä»¶è¯»å–é”™è¯¯

```
openslide.OpenSlideError: ...
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥æ–‡ä»¶æ ¼å¼
# æ”¯æŒçš„æ ¼å¼: .tif, .tiff, .ndpi, .vms, ç­‰

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´
import os
file_size = os.path.getsize('test.tif')
print(f"æ–‡ä»¶å¤§å°: {file_size / (1024**3):.2f} GB")

# ä½¿ç”¨ç»å¯¹è·¯å¾„
wsi_path = os.path.abspath('/path/to/slide.tif')
```

---

## ğŸ“ˆ æ€§èƒ½æœŸæœ›å€¼

åŸºäº RTX 3090, 512Ã—512 patches, batch_size=12:

| æ“ä½œ | é€Ÿåº¦ | å†…å­˜ |
|------|------|------|
| åŸå§‹æ¨ç† | ~45ms/patch | 2.5GB |
| ä¼˜åŒ–æ¨ç† | ~8ms/patch | 1.2GB |
| **åŠ é€Ÿå€æ•°** | **5.6x** | **-52%** |

### æ—¶é—´ä¼°ç®—

**å¤„ç† 1000 patches**:
- åŸå§‹: 45 ç§’
- ä¼˜åŒ–: 8 ç§’
- **èŠ‚çœ: 37 ç§’**

**å¤„ç†å¤§å‹ WSI (57,000 patches)**:
- åŸå§‹: ~8 å°æ—¶
- ä¼˜åŒ–: ~1.1 å°æ—¶
- **èŠ‚çœ: ~7 å°æ—¶**

---

## ğŸ“š è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰åå¤„ç†

```python
engine = OptimizedInference('UNet_17.pth')

# æ‰¹é‡é¢„æµ‹
patches = [Image.open(f'patch_{i}.png') for i in range(10)]
predictions = engine.predict_batch(patches)

# è‡ªå®šä¹‰å¤„ç†
for pred, patch in zip(predictions, patches):
    # åº”ç”¨è‡ªå®šä¹‰é˜ˆå€¼
    binary_pred = (pred > 0.5).astype(np.uint8)

    # ä¿å­˜
    result = Image.fromarray(binary_pred * 255)
    result.save('custom_result.png')
```

### æ‰¹é‡å¤„ç†å¤šä¸ª WSI

```python
import os
from pathlib import Path

wsi_dir = '/path/to/wsi/'
output_dir = '/path/to/output/'

engine = OptimizedInference('UNet_17.pth')

for wsi_file in os.listdir(wsi_dir):
    if wsi_file.endswith('.tif'):
        wsi_path = os.path.join(wsi_dir, wsi_file)
        output_path = os.path.join(output_dir, f'{wsi_file}_result.png')

        print(f"å¤„ç†: {wsi_file}...")
        engine.process_wsi_batched(wsi_path, output_path)
```

### ONNX éƒ¨ç½²

```python
# å¯¼å‡º
engine = OptimizedInference('UNet_17.pth')
engine.export_to_onnx('model.onnx')

# åœ¨å…¶ä»–ç¯å¢ƒä¸­ä½¿ç”¨
import onnxruntime as ort
session = ort.InferenceSession('model.onnx')

# æ¨ç†
import numpy as np
dummy_input = np.random.randn(1, 3, 512, 512).astype(np.float32)
outputs = session.run(None, {'input': dummy_input})
```

---

## âœ… æ£€æŸ¥æ¸…å•

è¿è¡Œå‰è¯·ç¡®è®¤:

- [ ] è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»
- [ ] ä¾èµ–åŒ…å·²å®‰è£… (`pip install -r requirements.txt`)
- [ ] æ¨¡å‹æ–‡ä»¶å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®
- [ ] WSI/patch æ–‡ä»¶å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®
- [ ] è¾“å‡ºç›®å½•å­˜åœ¨æˆ–å¯åˆ›å»º
- [ ] GPU æ˜¾å­˜å……è¶³ (æˆ–ä½¿ç”¨ CPU)

---

## ğŸ“ å®Œæ•´å·¥ä½œæµç¨‹

```bash
# 1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
.venv\Scripts\activate

# 2. è¿›å…¥ skills ç›®å½•
cd skills

# 3. ç¼–è¾‘é…ç½®æ–‡ä»¶
notepad example_optimized_inference.py
# ä¿®æ”¹æ¨¡å‹è·¯å¾„ã€WSI è·¯å¾„ç­‰

# 4. è¿è¡Œç¤ºä¾‹
python example_optimized_inference.py

# 5. æŸ¥çœ‹ç»“æœ
# è¾“å‡ºæ–‡ä»¶: wsi_result.png, unet_optimized.onnx

# 6. ï¼ˆå¯é€‰ï¼‰è¿è¡Œæ€§èƒ½æµ‹è¯•
python compare_inference_speed.py

# 7. æŸ¥çœ‹æ€§èƒ½å›¾è¡¨
# ç”Ÿæˆæ–‡ä»¶: inference_comparison.png
```

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

1. **æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£**: `skills/INFERENCE_OPTIMIZATION_README.md`
2. **æŸ¥çœ‹ Skill è¯´æ˜**: `skills/README.md`
3. **æŸ¥çœ‹é¡¹ç›®ç»“æ„**: `PROJECT_STRUCTURE.md`
4. **æŸ¥çœ‹ä¸»æ–‡æ¡£**: `README.md`

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼** ğŸš€

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥é˜…ä¸Šè¿°æ–‡æ¡£æˆ–æ£€æŸ¥é”™è¯¯æ—¥å¿—ã€‚
